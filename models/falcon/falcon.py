# -*- coding: utf-8 -*-
"""falcon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LpqKemUgmHwfl0I_nTKWKl5dyqGBs23x

# Set Up Environment
"""

!pip install transformers datasets peft

!pip install bitsandbytes==0.43.2

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_name = "tiiuae/falcon-7b-instruct"

# Configure 4-bit quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16  # Use float16 for computation
)

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    quantization_config=quantization_config,
    device_map="auto"  # Automatically distribute layers across GPU/CPU
)

# Ensure the tokenizer's pad token is set
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

"""# Load and Preprocess the Dataset:"""

import pandas as pd
from datasets import Dataset

# Load dataset
df = pd.read_csv("cleaned_dataset.csv")
dataset = Dataset.from_pandas(df)

# Tokenization
def preprocess_function(examples):
    inputs = [f"Client: {text}\nTherapist:" for text in examples["client"]]
    targets = [text for text in examples["therapist"]]
    model_inputs = tokenizer(
        inputs, text_target=targets, truncation=True, padding="max_length", max_length=512
    )
    return model_inputs

tokenized_dataset = dataset.map(preprocess_function, batched=True)

"""#  Fine-Tune Using LoRA"""

from peft import LoraConfig, get_peft_model

# Configure LoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["query_key_value"],  # Target attention layers
    lora_dropout=0.1,
    bias="none"
)
model = get_peft_model(model, lora_config)

"""# Set Training Arguments:"""

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=32,
    num_train_epochs=3,
    learning_rate=5e-5,
    fp16=True,
    save_strategy="epoch",
    logging_dir="./logs",
    report_to="none"
)

"""# training the model:"""

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)

trainer.train()

"""# save the finetuned model"""

# Save the model
model.save_pretrained("./fine_tuned_falcon")
tokenizer.save_pretrained("./fine_tuned_falcon")

# Save to Google Drive
from google.colab import drive
drive.mount('/content/drive')
model.save_pretrained('/content/drive/MyDrive/fine_tuned_falcon')
tokenizer.save_pretrained('/content/drive/MyDrive/fine_tuned_falcon')

"""# generate responses"""

# Set device to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def generate_response(input_text):
    prompt = f"Client: {input_text}\nTherapist:"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=50,
        temperature=01.5,
        top_p=0.9,
        repetition_penalty=1.2
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("Therapist:")[1].strip()

# Test inputs
test_input = "I've been feeling disconnected from my friends lately. what should i do?"
response = generate_response(test_input)
print(f"Input: {input_text}")
print(f"Response: {response}\n")

from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM
import torch

# Set device to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load the fine-tuned model and tokenizer
model_path = '/content/drive/MyDrive/fine_tuned_falcon'
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Load the model with 4-bit quantization
quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)
model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=quantization_config).to(device)

def generate_response(input_text, max_length=150):
    prompt = f"Client: {input_text}\nTherapist:"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,  # Set max_length to control the total output length
            temperature=1.5,
            top_p=0.95,
            repetition_penalty=1.2
        )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("Therapist:")[1].strip()

# Test input
test_input = "I feel like Iâ€™m not good enough for my family."
response = generate_response(test_input)
print(response)
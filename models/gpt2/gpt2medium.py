# -*- coding: utf-8 -*-
"""gpt2medium.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1onOLW9MGxXSHtSZmD952w8tAQjWQ76g_

# setup environment
"""

!pip install transformers datasets accelerate

from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from datasets import load_dataset, Dataset
import pandas as pd
import torch

"""# Load the Model and Tokenizer"""

# Use GPT-2 Medium
model_name = "gpt2-medium"

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Assign padding token
tokenizer.pad_token = tokenizer.eos_token

"""# Prepare Dataset"""

# Load your dataset
data = pd.read_csv("cleaned_dataset.csv")
dataset = Dataset.from_pandas(data)

# Split the dataset into train and validation sets
dataset = dataset.train_test_split(test_size=0.1)
train_dataset = dataset["train"]
eval_dataset = dataset["test"]

# Preprocessing function
def preprocess_function(examples):
    # Combine the client prompt and therapist response
    inputs = [f"Client: {prompt}\nTherapist: {response}" for prompt, response in zip(examples["client"], examples["therapist"])]
    tokenized = tokenizer(inputs, truncation=True, padding="max_length", max_length=128)
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

# Tokenize datasets
train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=["client", "therapist"])
eval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=["client", "therapist"])

"""# Cell 4: tokenize dataset"""

training_args = TrainingArguments(
    output_dir="./gpt2-medium-therapeutic",
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=2,  # Adjust for memory constraints
    per_device_eval_batch_size=2,
    num_train_epochs=5,
    save_strategy="epoch",
    save_total_limit=2,  # Save only the latest checkpoints
    logging_dir="./logs",
    logging_steps=10,
    load_best_model_at_end=True,
    fp16=True,  # Enable mixed precision for GPU
    push_to_hub=False
)

"""# Define Training Arguments"""

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer
)

# Start training
trainer.train()

"""# train the model:"""

# Save the fine-tuned model and tokenizer
model.save_pretrained("./gpt2-medium-therapeutic")
tokenizer.save_pretrained("./gpt2-medium-therapeutic")

# Save to Google Drive
from google.colab import drive
drive.mount('/content/drive')
model.save_pretrained('/content/drive/MyDrive/gpt2-medium-therapeutic')
tokenizer.save_pretrained('/content/drive/MyDrive/gpt2-medium-therapeutic')

# Define the device
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

# Function to generate responses
def generate_response(input_text):
    # Format the prompt
    prompt = f"Client: {input_text}\nTherapist:"
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=128).to(device)
    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=120,
        temperature=1,
        top_p=0.9,
        repetition_penalty=1.5,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )
    # Decode and clean the response
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "Therapist:" in response:
        response = response.split("Therapist:")[1].strip()
    response = response.split(".")[0].strip() + "."
    if not response.strip() or response == ".":
        response = "I'm here to listen and support you. You are not alone."
    return response

"""# save the finetuned model:"""

# Test inputs
test_inputs = [
    "I'm feeling so overwhelmed at work. What should I do?",
    "I can't stop thinking about my breakup. It's consuming me.",
    "I feel like I’m not good enough for my family.",
    "I’ve lost someone I deeply care about, and I can’t move on."
]

# Generate responses
for input_text in test_inputs:
    response = generate_response(input_text)
    print(f"Input: {input_text}")
    print(f"Response: {response}\n")

test_inputs = [
    "I've been feeling disconnected from my friends lately. What should I do?"
]

for input_text in test_inputs:
    response = generate_response(input_text)
    print(f"Input: {input_text}")
    print(f"Response: {response}\n")


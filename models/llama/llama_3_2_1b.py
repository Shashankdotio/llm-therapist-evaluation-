r # -*- coding: utf-8 -*-
"""LLAMA 3.2 1B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ObJidv4vCHHyVb2MRiUXGKSWriw9aWBb
"""

!pip install transformers torch

from huggingface_hub import login
login(token="your hf token")

from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the LLaMA 3.2 1B model
model_name = "meta-llama/Llama-3.2-1b"  # Correct model identifier
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

import torch

# Ensure the model has a pad_token_id set if not already
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id  # Using eos_token_id as pad_token_id

# Set device: Use CUDA (GPU) if available, otherwise fallback to CPU
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# Function to generate a single therapeutic, empathetic response
def generate_therapeutic_response(input_text):
    # Refined prompt emphasizing a single response
    prompt = f"""
    You are a compassionate therapist. Respond with one empathetic and
    supportive message to help the client feel heard and validated.

    Client: '{input_text}'

    Therapist:
    """

    # Tokenize the input and create the necessary tensors
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    # Generate the response
    with torch.no_grad():
        output = model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_length=150,  # Shorter max length for a concise response
            pad_token_id=tokenizer.pad_token_id,
            temperature=0.6,  # Lower temperature for focused response
            top_p=0.85,       # Tighten nucleus sampling for relevancy
            top_k=40,         # Restrict top-K sampling
            no_repeat_ngram_size=3
        )

    # Decode and return the response
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response

# Example usage
input_text = "My dog died, and I'm feeling so sad. I miss him so much."
response = generate_therapeutic_response(input_text)
print(response)

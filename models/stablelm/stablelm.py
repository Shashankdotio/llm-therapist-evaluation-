# -*- coding: utf-8 -*-
"""stableLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n-RfKmqamnpcPxJTUG1mQXz0M_SVdiMC

# setup environment
"""

!pip install torch transformers datasets accelerate peft trl bitsandbytes

from huggingface_hub import login
login(token="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")  # Replace with your actual token

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model

"""# Load the Model and Tokenizer"""

# Load the DeepSeek model (adjust model size as needed)
model_name = "stabilityai/stablelm-3b-4e1t"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Ensure pad_token_id is set
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

# Set device: Use CUDA if available
import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

"""# Configure LoRA for Efficient Fine-Tuning"""

# Configure LoRA for parameter-efficient fine-tuning
config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],  # Target attention modules
    lora_dropout=0.1,
    bias="none"
)
model = get_peft_model(model, config)

"""# Load & Preprocess Custom Dataset"""

import pandas as pd
from datasets import load_dataset

# Load dataset from CSV
df = pd.read_csv("cleaned_dataset.csv")
print(df.head())

# Convert to Hugging Face Dataset
dataset = load_dataset("csv", data_files="cleaned_dataset.csv", split="train")

# Tokenization function
def preprocess_function(examples):
    inputs = [f"Client: {inp}\nTherapist: " for inp in examples["client"]]
    model_inputs = tokenizer(
        inputs, text_target=examples["therapist"], truncation=True, padding="max_length", max_length=512
    )
    return model_inputs

# Tokenize dataset
tokenized_dataset = dataset.map(preprocess_function, batched=True, batch_size=16)

"""# Define Training Arguments"""

# Split dataset into training and validation sets
train_test_split = tokenized_dataset.train_test_split(test_size=0.1)
train_dataset = train_test_split["train"]
val_dataset = train_test_split["test"]

"""# train the model:"""

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=16,
    learning_rate=5e-5,
    num_train_epochs=3,
    fp16=True,
    logging_dir="./logs",
    save_strategy="epoch",
    report_to="none",
    remove_unused_columns=False  # <-- Add this to avoid column mismatch error
)

# Save to Google Drive
from google.colab import drive
drive.mount('/content/drive')
model.save_pretrained('/content/drive/MyDrive/StableLM')
tokenizer.save_pretrained('/content/drive/MyDrive/StableLM')

from google.colab import drive
drive.mount('/content/drive')

# Define the device
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

# Define response generation function
def generate_response(input_text):
    prompt = f"""
   You are a compassionate therapist. Respond with empathetic, supportive, and validating responses that directly address the client's feelings. Focus on making the client feel understood and comforted.

    Client: {input_text}
    Therapist:
    """
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
    outputs = model.generate(
        input_ids=inputs['input_ids'],
        attention_mask=inputs['attention_mask'],
        max_new_tokens=120,
        temperature=0.5,
        top_p=0.95,
        top_k=50,
        no_repeat_ngram_size=3,
        repetition_penalty=2.0,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "Therapist:" in response:
        response = response.split("Therapist:")[1].strip()
    response = response.split(".")[0].strip() + "."
    if not response.strip() or response == ".":
        response = "I'm here to listen and support you. You are not alone."
    return response

"""# save the finetuned model:"""

# Test inputs
test_inputs = [
    "I'm feeling so overwhelmed at work. What should I do?",
    "I can't stop thinking about my breakup. It's consuming me.",
    "I feel like I’m not good enough for my family.",
    "I’ve lost someone I deeply care about, and I can’t move on."
]

# Generate responses
for input_text in test_inputs:
    response = generate_response(input_text)
    print(f"Input: {input_text}")
    print(f"Response: {response}\n")

test_inputs = [
    "i failed in my test, im sad, how do i cope up?"
]

for input_text in test_inputs:
    response = generate_response(input_text)
    print(f"Input: {input_text}")
    print(f"Response: {response}\n")

!pip install gguf

# Save the fine-tuned model
model.save_pretrained("./fine_tuned_stableLM")
tokenizer.save_pretrained("./fine_tuned_stableLM")